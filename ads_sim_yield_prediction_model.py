# -*- coding: utf-8 -*-
"""ADS SIM Yield Prediction Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W1DGG0PdajOsxyvjNCFNapT42OHRTLbt
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')
# load integrated dataset
filename = "/content/drive/My Drive/Colab Notebooks/ADS PROJECT/SIM DATA/integrated_okra_data.csv"
data = pd.read_csv(filename)

print(data.dtypes)

# descriptive stats
descriptive_stats = data.describe()
print(descriptive_stats)

# missing value check
missing_values = data.isnull().sum()
print(missing_values)

# choose features for distribution plots
features_to_plot = ['Yield (kg/ha)', 'Growing Period (days)', 'Leaf Area Index (LAI)', 'Crop Water Use (mm/day)', 'Soil Moisture Content', 'LAI Rate of Change']

# histograms + summary stats for each selected feature
for feature in features_to_plot:
    plt.figure(figsize=(10, 6))
    sns.histplot(data[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()
    print(f"Summary Statistics for {feature}:\n{data[feature].describe()}\n")

# box plot + summary stats
for feature in features_to_plot:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=data[feature])
    plt.title(f'Box Plot of {feature}')
    plt.xlabel(feature)
    plt.show()
    print(f"Summary Statistics for {feature}:\n{data[feature].describe()}\n")

# correlation analysis
correlation_matrix = data.corr()

# show correlation matrix as heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# show summary stats of correlation matrix
correlation_matrix_summary = correlation_matrix.describe()
print(correlation_matrix_summary)

# extract features from 'Planting Date', 'Harvest Date', and 'Date'
data['Planting Date'] = pd.to_datetime(data['Planting Date'])
data['Harvest Date'] = pd.to_datetime(data['Harvest Date'])
data['Date'] = pd.to_datetime(data['Date'])  # If this column is relevant

# extract additional date features
data['Planting Day of Week'] = data['Planting Date'].dt.dayofweek
data['Harvest Day of Week'] = data['Harvest Date'].dt.dayofweek
data['Day of Week'] = data['Date'].dt.dayofweek

# drop original date columns
data.drop(['Planting Date', 'Harvest Date', 'Date'], axis=1, inplace=True)


# drop original 'Nutrient Uptake' column
data.drop(['Nutrient Uptake'], axis=1, inplace=True)


# one-hot encode categorical variables
categorical_columns = ['Variety', 'Soil Type', 'Growth Stage', 'Pest/Disease Incidence', 'Weather Data Correlation']
one_hot_encoder = OneHotEncoder(sparse=False)
encoded_categorical_data = one_hot_encoder.fit_transform(data[categorical_columns])

# create DataFrame from encoded data
encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=one_hot_encoder.get_feature_names_out(categorical_columns))

# drop original categorical columns and concatenate encoded DataFrame
X = pd.concat([data.drop(categorical_columns, axis=1), encoded_categorical_df], axis=1)
X = X.drop('Yield (kg/ha)', axis=1)  # excluding target column


# define the target variable
y = data['Yield (kg/ha)']

# initialize random forest regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# initialize gradient boosting regressor
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)

# fit models
rf.fit(X, y)
gb.fit(X, y)

# get feature importances for both models
rf_importances = rf.feature_importances_
gb_importances = gb.feature_importances_

# convert importances into DataFrames
rf_feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf_importances})
gb_feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': gb_importances})

# sort DataFrames by importance
rf_feature_importances = rf_feature_importances.sort_values('Importance', ascending=False)
gb_feature_importances = gb_feature_importances.sort_values('Importance', ascending=False)

# no. of features to display for top and bottom importance
n_display = 5

# random forest feature importances
rf_top_features = rf_feature_importances.head(n_display)
rf_bottom_features = rf_feature_importances.tail(n_display)

# gradient boostin feature importances
gb_top_features = gb_feature_importances.head(n_display)
gb_bottom_features = gb_feature_importances.tail(n_display)

# plot and show summary for random forest
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=rf_feature_importances)
plt.title('Random Forest Feature Importances')
plt.show()
print("Random Forest Top Features:\n", rf_top_features, "\n")
print("Random Forest Bottom Features:\n", rf_bottom_features, "\n")

# plot and show summary for gradient boosting
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=gb_feature_importances)
plt.title('Gradient Boosting Feature Importances')
plt.show()
print("Gradient Boosting Top Features:\n", gb_top_features, "\n")
print("Gradient Boosting Bottom Features:\n", gb_bottom_features, "\n")

# split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# fit models on train
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)

# predict on test
rf_predictions = rf.predict(X_test)
gb_predictions = gb.predict(X_test)

# evaluate models
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_rmse = np.sqrt(rf_mse)
rf_r2 = r2_score(y_test, rf_predictions)

gb_mse = mean_squared_error(y_test, gb_predictions)
gb_rmse = np.sqrt(gb_mse)
gb_r2 = r2_score(y_test, gb_predictions)

# print evaluation metrics
print("Random Forest: MSE =", rf_mse, "RMSE =", rf_rmse, "R2 =", rf_r2)
print("Gradient Boosting: MSE =", gb_mse, "RMSE =", gb_rmse, "R2 =", gb_r2)

# cross-val for random forest
rf_cv_scores = cross_val_score(rf, X, y, cv=5, scoring='r2')
print("Random Forest Cross-Validation Scores:", rf_cv_scores)

# cross-val for gradient boost
gb_cv_scores = cross_val_score(gb, X, y, cv=5, scoring='r2')
print("Gradient Boosting Cross-Validation Scores:", gb_cv_scores)